# 环境及测试
## 准备工作

```
1. 安装Ollama
2. Python 3.10+
```
### 下载模型
```bazaar
# 嵌入模型
ollama pull nomic-embed-text

# 对话模型
ollama pull qwen2.5:0.5b
```

### ollama环境变量（推荐）
```
OLLAMA_HOST=0.0.0.0 解决外网访问问题

OLLAMA_MODELS=E:\ollamaimagers   解决模型默认下载C 盘的问题

OLLAMA_KEEP_ALIVE=24h     设置模型加载到内存中保持24个小时(默认情况下，模型在卸载之前会在内存中保留 5 分钟)

OLLAMA_HOST=0.0.0.0:8080  解决修改默认端口11434端口

OLLAMA_NUM_PARALLEL=2  设置2个用户并发请求

OLLAMA_MAX_LOADED_MODELS=2 设置同时加载多个模型
```

### 启动ollama并加载模型
```bazaar
C:\Users\Lenovo>ollama run qwen2.5:0.5b

# 加载模型
>>> /load qwen2.5:0.5b
Loading model 'qwen2.5:0.5b'
```

### 测试代码
```python
from langchain_community.chat_models.ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# 实例化
llm = ChatOllama(base_url="http://localhost:11434", model="qwen2.5:0.5b")

# 声明提示词模板
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "你是一个专业的翻译官，可以将用户提供的内容翻译为英文"),
        ("user", "{question}")
    ]
)
# 创建链
# prompt_template:构建的提示词模板
# llm:实例对象
# StrOutputParser:输出解析器
chain = prompt_template | llm | StrOutputParser()

# 调用链
result = chain.invoke({"question": "我爱写代码"})

print(result)
# 运行结果
I love to write code.
```